{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c68d3b55",
   "metadata": {},
   "source": [
    "# Data Leakage - # Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "68d2803a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pedrojosetrujillomejia/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/pedrojosetrujillomejia/opt/anaconda3/lib/python3.9/site-packages/sklearn/impute/_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n",
      "/Users/pedrojosetrujillomejia/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No potential data leakage from Age into target variable\n",
      "No potential data leakage from Flight Distance into target variable\n",
      "Model score: 0.6505003849114703\n",
      "Predicted target values:\n",
      "['neutral or dissatisfied', 'satisfied', 'neutral or dissatisfied', 'satisfied', 'satisfied', ..., 'satisfied', 'neutral or dissatisfied', 'satisfied', 'satisfied', 'neutral or dissatisfied']\n",
      "Length: 5196\n",
      "Categories (3, object): ['dissatisfied', 'neutral or dissatisfied', 'satisfied']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pedrojosetrujillomejia/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/pedrojosetrujillomejia/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "df = pd.read_csv('/Users/pedrojosetrujillomejia/Desktop/test.csv')\n",
    "\n",
    "\n",
    "df['satisfaction'] = pd.Categorical(df['satisfaction'], categories=['dissatisfied', 'neutral or dissatisfied', 'satisfied'])\n",
    "df['satisfaction'] = df['satisfaction'].cat.codes\n",
    "\n",
    "\n",
    "target = 'satisfaction'\n",
    "\n",
    "\n",
    "correlated_features = ['Age', 'Flight Distance']\n",
    "\n",
    "\n",
    "categorical_cols = ['Gender', 'Class', 'Type of Travel', 'Customer Type']\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded_cols = encoder.fit_transform(df[categorical_cols])\n",
    "df_encoded = pd.concat([df.drop(categorical_cols, axis=1), pd.DataFrame(encoded_cols.toarray())], axis=1)\n",
    "\n",
    "\n",
    "train_df, test_df = train_test_split(df_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent', add_indicator=True)\n",
    "X_train = imputer.fit_transform(train_df.drop(target, axis=1))\n",
    "y_train = train_df[target]\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "X_test = test_df.drop(target, axis=1)\n",
    "y_test = test_df[target]\n",
    "\n",
    "\n",
    "for feature in correlated_features:\n",
    "    if X_test[feature].dtype == 'object':\n",
    "        X_test[feature] = pd.to_numeric(X_test[feature], errors='coerce')\n",
    "    corr = X_test[feature].astype(float).corr(y_test.astype(float))\n",
    "    if corr > 0.9:\n",
    "        print(f\"WARNING: Potential data leakage from {feature} into target variable\")\n",
    "    else:\n",
    "        print(f\"No potential data leakage from {feature} into target variable\")\n",
    "\n",
    "\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "y_pred = model.predict(X_test_imputed)\n",
    "\n",
    "\n",
    "y_pred_categorical = pd.Categorical.from_codes(y_pred, categories=['dissatisfied', 'neutral or dissatisfied', 'satisfied'])\n",
    "\n",
    "\n",
    "score = model.score(X_test_imputed, y_test)\n",
    "print(f\"Model score: {score}\")\n",
    "print(\"Predicted target values:\")\n",
    "print(y_pred_categorical)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bf57d4",
   "metadata": {},
   "source": [
    "# Data Contimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "be30e361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Number of features in X_train and X_test are different, scaling may not work properly.\n",
      "No potential data leakage from Inflight wifi service into target variable\n",
      "No potential data leakage from Ease of Online booking into target variable\n",
      "No potential data leakage from On-board service into target variable\n",
      "No potential data leakage from Cleanliness into target variable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pedrojosetrujillomejia/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/pedrojosetrujillomejia/opt/anaconda3/lib/python3.9/site-packages/sklearn/impute/_base.py:49: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = stats.mode(array)\n",
      "/Users/pedrojosetrujillomejia/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "file_path = '/Users/pedrojosetrujillomejia/Desktop/test.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "target = 'satisfaction'\n",
    "\n",
    "\n",
    "\n",
    "df['satisfaction'] = pd.Categorical(df['satisfaction'], categories=['dissatisfied', 'neutral or dissatisfied', 'satisfied'])\n",
    "df['satisfaction'] = df['satisfaction'].cat.codes\n",
    "\n",
    "\n",
    "\n",
    "categorical_cols = ['Gender', 'Class', 'Type of Travel', 'Customer Type']\n",
    "\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "encoded_cols = encoder.fit_transform(df[categorical_cols])\n",
    "df_encoded = pd.concat([df.drop(categorical_cols, axis=1), pd.DataFrame(encoded_cols.toarray())], axis=1)\n",
    "\n",
    "\n",
    "Q1 = df_encoded.quantile(0.25)\n",
    "Q3 = df_encoded.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df_filtered = df_encoded[~((df_encoded < (Q1 - 1.5 * IQR)) | (df_encoded > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "\n",
    "\n",
    "train_df, test_df = train_test_split(df_filtered, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent', add_indicator=True)\n",
    "X_train = imputer.fit_transform(train_df.drop(target, axis=1))\n",
    "y_train = train_df[target]\n",
    "\n",
    "\n",
    "drop_column = set(test_df.columns) - set(train_df.columns)\n",
    "X_test = test_df.drop(target, axis=1).drop(drop_column, axis=1)\n",
    "y_test = test_df[target]\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Check number of features in X_train and X_test before scaling\n",
    "if X_train.shape[1] != X_test.shape[1]:\n",
    "    print(\"Warning: Number of features in X_train and X_test are different, scaling may not work properly.\")\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Scale X_test only if the number of features is the same as X_train\n",
    "if X_train.shape[1] == X_test.shape[1]:\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "correlated_features = ['Inflight wifi service', 'Ease of Online booking', 'On-board service', 'Cleanliness']\n",
    "\n",
    "for feature in correlated_features:\n",
    "    if X_test[feature].dtype == 'object':\n",
    "        X_test[feature] = pd.to_numeric(X_test[feature], errors='coerce')\n",
    "    corr = X_test[feature].astype(float).corr(y_test.astype(float), method='pearson', min_periods=1)\n",
    "    if pd.isna(corr):\n",
    "        print(f\"WARNING: Could not calculate correlation for feature {feature}\")\n",
    "    elif corr > 0.9:\n",
    "        print(f\"WARNING: Potential data leakage from {feature} into target variable\")\n",
    "    else:\n",
    "        print(f\"No potential data leakage from {feature} into target variable\")\n",
    "\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9994683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
